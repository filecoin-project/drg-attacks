# Basic intuitions behind the current code

[What do we aim to do here?]

This text uses the definitions from section 2 of the [Practical Graphs for Optimal Side-Channel Resistant
Memory-Hard Functions](https://acmccs.github.io/papers/p1001-alwenA.pdf) paper (our canonical reference), where `DRSample` is first described. It is also recommended to read the introduction in section 1 for a general background on DRGs since this is not a self-contained document. On top of those we may build new concepts and definitions (but we will not follow any of the proofs from other sections).

These are a collection of hypothesis fitted into a single narrative **without any proof backing them**. All the claims and formulas are mere simplifications, their main purpose is to lay out some intuitions to simplify the approach to the DRG problem.

Any comments inside brackets (`[...]`) are meta-comments about the text itself and can be safely ignored (especially recommended during the first read).

## Two ways to reduce depth

### Hard split

The most evident approach, drops the depth right down in two (assuming we split it near the middle). [The term *hard* is used only for emphasis and to distinguish it from any other accidental use of the term *split* alone.]

### Exchange nodes

We start considering the initial immediate predecessors path (IPP), which has a length equal to the number of nodes `n` (giving the best possible depth) and all edges of distance one. We consider a sub-path of it, a *span* of consecutive nodes, and look for longer edges contained there. Within that span is easy to see that when we remove a node and interrupt the IPP a longer edge will be taken (instead of the one with length one) and some nodes will be dropped from the *main path* that determines the depth of the graph. Main path is a synonym to the largest path in the graph, wanting to emphasize that the main one won't be changing in any major way (no split), we will just be exchanging minor amounts of nodes.

The nodes dropped will be the ones in the edge *interval* (all the nodes bigger than the edge parent and smaller than the child, without including them) of the smallest edge that can go around the cut generated by removing a node (i.e., the smallest edge whose interval contained the removed node). We emphasize on the *exchange* term because these nodes, that don't contribute to the graph depth now, are *not* removed, they are still part of the graph, we just exchanged them out of the main path for a longer edge "jumping them", but they can easily be brought back into the main path if other nodes are removed that might make them "useful" again (participate in a larger depth). Alternatively we can use the *switching* edges terminology: we switched from an edge of length 1 to another of higher length, the difference will be subtracted form the final depth.

[Diagram with basic example, and then with another one that has edges in the exchanged nodes.]

This concept is the most important one of this text because everything else follows pretty directly from it, but it is also the most brittle because it's **inaccurate and sub-optimal**. We will normally be focusing on the effect of removing a single node from a reduced span and assume that the edge taken to bypass it will end up in the final depth of the graph, without taking into consideration the interactions with other nodes removed (from contiguous spans nearby). In practice this is not true, the switched edges don't end up in the final largest path, and the amount of depth reduction is less than expected, but those effects (up to now) seem minor and can be contained to a certain degree. This is a *local* attack that assumes that it can be aggregated as just the sum of its parts to achieve a certain `(e. d)` pair whereas most attacks try to optimize for the global view of the graph.

[Eventually would like to converge both attacks into a unifying theory but at the moment it seems simple to think of them separately.]

### Metrics

**DER**: depth exchange ratio, `(1-d)/e`, is the ratio between the reduction in depth and the nodes removed (size of set S), not exactly the same as the `e*d` we're using now, but still captures the essence of what we care about in robustness. This is by definition `>= 1` because the depth of the graph is reduced at least by one per each node removed from the graph itself. We're borrowing on the meaning of *exchange* from before in the sense that we trade nodes for depth.

**NER**: node exchange ratio, the amount of nodes taken out of the main path per one node removed (always `>= 1`). In an ideal scenario, if we don't do a hard split, this number would be equal to the DER, and sometimes we will refer to them interchangeably; we will make the distinction mostly in attacks analysis and implementation, having the DER as the ideal we want to arrive by means of exchanging nodes, the NER, with well crafted exchanges.

**SC**: split cost, the amount of nodes (relative to `n`, as `e`) that need to be removed to generate a hard split near the middle of the main path to drop the depth in half. The resulting depth in this case is `(1-SC)/2`, assuming that the nodes removed don't generate any exchanges (keeping the NER at 1) and just reduce the original main path's depth by the same amount (this is not always the case), after which the depth is reduced in two. The DER is then (with `e = SC`, only removing the nodes necessary for the split) `1-[(1-SC)/2]/SC` (we will rework this later when needed, for now we will just use it as is replacing SC with the appropriate value).

After a hard split the NER also drops in half, meaning that to reduce the depth even further we need to reduce the depths from the two remaining halves (there is no longer a main path) having to remove twice the amounts of nodes we would have in a main-path scenario. Normally when we refer to the NER we are analyzing it in the context of a main path, we don't focus on interactions of different attacks (e.g., we always focus on the first node we remove and its impact on depth, but not how it interacts with other nodes removed elsewhere).

[Absolute vs Relative. Split is always relative, but the exchange has a component of absoluteness we need to describe.]

## Uniform graph

We present here a basic construction to expand on the ways to reduce depth described before. Any graph construction we discuss throughout the document will have, as a minimum, the IPP (a path traversing all nodes connecting them with their immediate predecessors, guaranteeing an initial depth of `n`), and we will normally not count it in the degree `m`.

### Protection against a hard split

As a starting point suppose we want to avoid a hard split and we assume that the attacker can't remove more than a certain fraction of `e` nodes from the graph (in order to make the attack rational, this restriction will be lifted later). As an example let's set that removal limit at `e = 0.2` (our *target* `e`), so we want the graph to hold for any amount of removal up to that, meaning the SC will be `0.2`. The most straightforward approach is to add edges of intervals of that size, so even if the attacker concentrates all the removal nodes in a single place they will still be contained within one of these edges.

We will set this SC-interval edges for *all* nodes, and not something like back to back or in an interspersed layout as one might initially think, otherwise splitting would take less than SC nodes because we would just need to take out the nodes that have those edges (that might be lower than SC). This is the first insight we can collect, protecting against a certain removal of consecutive nodes, which we'll call *blocks* (similar to the paper but without any theoretical proof implications), means we need to protect all nodes with a particular type of edge. [Expand on why with a concrete graphical example, explain why something even as dense as having them every other node wouldn't work.]

For simplicity we will be ignoring, in this protection, the fact that the middle of the graph is more "valuable" than its extremes, as a hard split in the middle will yield a higher DER than something at, say, 90% of the graph (which would result in a main path of 0.9 where further attacks should be concentrated and a path of length `0.1` that could largely be ignored). This seems like it would otherwise be the only motivation to concentrate more edges in the center than anywhere else (instead of the uniform layout we're doing here).

### Protection against node exchanges

In the current topology, consisting of the IPP of edges of length one (and interval zero, because there's no "space" in the middle) and edges of interval SC, even if we can't split the graph we can remove a single node to force the main path to take the big SC-interval edge and drop then SC nodes from the depth (having a NER of SC, we removed only one node and reduced the depth by SC). Building on the previous example graph of `SC = 0.2`, it would take just 5 nodes (interleaved regularly SC nodes apart) to force the main path to take only SC-interval edges and have a resulting depth of just 5 nodes (more precisely 5 edges and 6 nodes) which will yield a much higher DER than just causing the split itself.

The next step then should be to protect from this attack by including other smaller-interval edges. To continue the example suppose we want to enforce a NER of 4 (meaning we think the attack won't be profitable if the attacker has to spend more than that ratio to reduce depth). [The imposition of NER and SC limits makes the example simpler but may not adhere with our attacker model.] In that case we will need edges of interval 4 so each node we remove from the graph, instead of forcing a switch to the SC-interval edges (with astronomical NERs) will switch to this smaller-interval edges that won't cause the drop of so many nodes from the main path.

Analogous to the previous reasoning the attacker could now remove 4 nodes at a time, effectively removing this level of protection and causing the switch to the SC-interval edges. As the SC is still much bigger than the 4 nodes the attacker had to remove to trigger the switch, it still renders a NER much higher than 4. To protect against that we will need edges with a interval of `NER^2`, in this example 16, so when the attacker removes the 4-interval edges (by using a removal block of size 4) it will have exchanged them for the drop of only 16 nodes from the main path (keeping the NER constant at the target 4).

Although not highlighted up to now, note also the need to have these edges in *all* nodes, if there was a span of 4 nodes with less than 4 edges we wouldn't need to remove all 4 nodes to break out and switch to the 16-interval edges, having a NER bigger than 4 (e.g., if we only set the 4-interval edges every other node we would need to remove only 2 nodes to break out and switch to a 16-interval edge with a NER of `16/2 = 8`). [This needs to be visualized in a diagram, how all 4 edges from consecutive nodes will protect the same adjacent node.]

The same problem will be presented then for the removal of `NER^2` blocks with the same type of protection needed (`NER^3`), and so on. For each of those groups of same-interval edges we can define a `scale` of the protections:   scale `1` protecting against removal of 1 node with edges of `NER^1` intervals, the scale `2` will be the removal of blocks of size `NER^1` nodes and needing the protection of `NER^2` and so on. Generalizing, each protection `scale` uses edges of `NER^scale` intervals to protect against the removal of blocks with size `NER^(scale-1)`, with `scale` belonging to the range `[1,m]`, that is, the largest interval will be `NER^m`. The edge intervals then grow exponentially and will stop once we reach the final SC-interval edges that were in place to protect against a hard split, which can now be thought of as the last protection scale.

[We can think of this also in terms of meta-graphs as described in the paper, but this may bring many conceptual implications from it that would make this explanation unnecessarily complex.]

To secure this protection model we would need then `log_{NER}(SC * n)` (logarithm of `SC` with base `NER`) edges *per node* (`m`). Any scale that we leave unprotected will allow the attacker the desired NER, achieved by removing blocks of only that size (also, failing to reach the SC top interval will make the split cheaper than desired). It's important to notice that any scale is as important (or weak) as any other, the attacker can easily drop the *same* depth by *any* block size, either single nodes or gigantic blocks of, say, 1/10th of the graph (in total, we always remove the same amount of nodes, is a matter of how we lay them out).

Since the SC is a relative amount (as `e`), the actual number of nodes needed for the split will grow with the graph size `n`, which means that the number of protection scales needed will also grow, and hence then number of edges per node, that is, its degree `m`. This is probably the most important insight of this entire document:

> To keep a certain level of robustness (DER) constant, the node's degree will need to grow *logarithmically* with the graph size (it should be proportional then to `k`, with the size defined as `n = 2^k`).

The *attack surface* of the graph grows with its size. If we keep a degree constant it may give the deceitful impression that the robustness will be constant because as the size grows so does the number of edges (proportionally to it), but as we can see a bigger graph means new scales of protection needed for a certain robustness (DER) to hold because there are new levels of exchange. The increase of total edges with fixed `m` just guarantees that the entire span of the graph will be protected against the *old* scales of attack ("width protection") as it grows, but not for new scales ("depth protection" [don't use the term depth here]). Put another way, as the graph grows we have more ways of interspersing the removal nodes `e` across it. [Add a diagram if needed, this is a very important concept.]

This is in part derived from the fact that we care about relative `(e,d)` values, and as the graph grows so does the depth of the main path we need to protect while the attacker also has more `e` nodes available to remove, so protecting that robustness gets more difficult. As a simple example suppose a basic construction that just has the IPP. For a fixed size of that graph, we just need to remove one node to drop the depth in half: the middle node; just one node, independent of the size of the graph. (Note how depth keeps being relative to the size but the amount of nodes we need to remove is absolute, we can exploit this to go further). If we now double the size, we will have, for a fixed `e`, double the nodes we can remove from the graph, so we can cut the depth in half again, the `e` stays fixed but the `d` dropped in half again (some rounding errors in this explanation are ignored to keep it simple). For concreteness, suppose we have 10 nodes and an `e` of 0.1, and we will have a resulting depth `d` of 0.5. If we now triple that size, 30 nodes, `e` can stay in 0.1 while having now 3 nodes to remove, we can take the middle one (1 node) to drop the depth in half to 0.5, and then cut the remaining halves in half again (2 nodes, one for each half), dropping the `d` to 0.25. The logarithmic component is manifested here in the fact that, as we want to keep reducing the depth again and again we will need every time (roughly) twice as much `e` nodes as before (so we need to double the graph size to get them) because we have more and more scraps of graph to cut in half again.

### Balancing NER and SC

Finally, to complete the construction we should note that we need to balance the robustness that comes from splitting the graph (SC) and exchanging nodes (NER). If one of those is cheaper than the other it will dominate the DER because the attacker will go for the weaker one (this is the main motif across the text, asymmetries cause weak spots which the attacker can focus on, the system as a whole is as strong as its weakest spot). There's a trade-off between NER and SC, strengthening one weakens the other.

Let's consider again the SC of `0.2`, by itself it will bring down the depth (as explained before) to `(1-0.2)/2 = 0.4`, having a DER of `(1-0.4)/0.2 = 3`, meaning that I can reduce the depth by 3 times the amount of nodes remove. If we had set up an NER of 4, it will mean then that, even if we lift the removal limit of the attacker (of `0.2`), it will still go for the NER attack because it has a higher DER than the split. We've made the split too secure at the cost of the NER, this is so because, for a fixed `m`, the NER is determined by the `log_{NER}(SC * n)`: how many times I raised NER to reach the SC. If the SC is too high we need higher jumps of NER to reach it (for a fixed `m`), leaving a higher DER for the attacker. Conversely, if we lower the NER to make it more secure we'll be lowering the SC making the split cheaper (also rising the DER, this time do to the change of SC instead of the NER).

### Theoretical DER: decay of robustness with graph size

With the SC/NER balanced into a unifying DER we can now put more concrete numbers to the insight previously mentioned of how does robustness decays with graph size (i.e., how does DER increase with `n`). Balancing both types of attacks would mean then that the DER of both NER and SC match: `DER(SC) = DER(NER)`, with  `DER(SC) = 1-[(1-SC)/2]/SC`, simplifying (just a bit, we don't want to lose the concepts behind of each term) we have `DER(SC) = (0.5 - SC/2) / SC`: we lower the depth by a fixed 0.5 (splitting the main path in two) and also by the amount of SC nodes actually removed (divided by two because they represent the NER part of the attack, which also drops its effectiveness by half when there are two paths we need to remove nodes from), all that compared to the `SC` the attacker has to remove. In an ideal scenario we'll also have `DER(NER) = NER`, which in this case is actually accurate because the uniformity of the protections allow also a uniform attack without interactions between different exchanges (if everything is balanced there's no motivation to interact between different edges because that won't improve graph depth without hurting it in another part by the same magnitude). [Show a diagram of a counterexample with an irregular topology.]

[Review and simplify the narrative of these last formulas.]

Equating both DERs (now a single one) gives `SC = 1 / (2*DER + 1)`, and replacing SC with the NER (now just DER) from the logarithmic formula previously discussed (when both attacks are balanced) as `DER^m = SC * n`  will result in  `DER^m / n = 1 / (2*DER + 1)`, ignoring the `+ 1` term for the moment we have `DER^(m+1) = n/2`. [Are we going too fast here?] The interpretation of the formula is as follows. We simplified the `DER(SC)` as just `1/2` (the `+1` ignored, which is negligible as the DER grows). We have `m` scale protections, the highest one with edge intervals of `NER^m`, e.g., we can remove a block of `NER^(m-1)` nodes to switch to a *final* `NER^m`-interval edge, keeping the DER at the NER constant. If we were to remove that last protection, with a block of `NER^m` nodes, we would achieve a hard split, the DER would be the one form SC (and not NER) in this case. But since those two are balanced we still expect a DER of NER (just as if another `m+1` scale existed with `NER^(m+1)` intervals). To keep the DER equal we need that that hypothetical protection edge with interval `NER^(m+1)`, that doesn't exist, to be now what we win from the hard split, namely `n/2` nodes (otherwise `NER^m` blocks would be more profitable than lower order ones). [This is the closer we have to unifying the NER/SC stories into a single one under DER, even if we could retell it entirely using just that, the road traveled seems worth its insights at the moment.]

To better understand the decay of robustness with graph size we rewrite the formula expressing `n` in terms of its logarithm `k` (`n = 2^k`) and expressing the root as an exponent:

> `DER(m, k) = 2^[(k - 1) / (m + 1)]`

For a fixed `m` the increase of DER (drop of robustness) will be exponential (conversely logarithmical, as expressed in the Theorem 3.1 of the cited paper) with `k` at a rate dictated by the `m`, so the DER will grow one order of magnitude per `m + 1` orders of magnitude that the graph size increases (*order of magnitude* meant in the binary system of powers of two). As an example, for our typical construction with an `m` of 6 we would have a DER of (approximately) 2.4 for a `k` of 10, 6.6 for a `k` of 20 and 17.7 for a `k` of 30, see this [graph](https://www.desmos.com/calculator/hg7qb2ead6) for more details of how the `m` impacts the evolution of the DER (keeping in mind that the `k` represents a logarithmic scale of the size, which justifies the steepness of the curve).

Note that throughout this analysis we've analyzed only the first hard split of the graph, this is because subsequent splits have lower DERs so even if we have the first one balanced with the NER the rest will always be less profitable that keep attacking through exchanges.

This is the theoretical DER offered by a Uniform graph of size `n` (`k`) and degree `m`, it is also very much practical because implementing and testing is pretty straightforward (and almost pointless if we don't have an appropriate attack for it that doesn't depend on asymmetries). It is left to find out if this can be related to the theoretical `DER(m, k)` of *any* graph construction, we can only speculate at the moment but the rationale is that all the attacks we've seen so far (Greedy/Valiant and its variants) rely on asymmetries that this graph does not expose.

[We are actually not counting at the moment that to jump from one scale to the next we need the `interval` plus one node, as `interval` alone is just bypassed by edges of that scale, e.g., if the NER is 4, I have edges of `interval` 4 and `length` 5, and I need to remove 5, the `length`, not 4, the `interval`.]

## DRSample

We now study the `DRSample` construction (defined in section 3 of the paper cited) which is the default DRG implementation used at the moment. The development of the Uniform graph and the insights behind it had as an objective to have a different construction to compare against and draw some conclusions from it.

The first thing to notice is the similarity in the exponential/logarithmic distributions of the edges generated (although I haven't followed the proof of Theorem 3.1 thoroughly it probably uses some intuitions similar to the ones we described here, like block-depth-robustness which I think can relate to the previously discussed removal of `NER^scale` blocks at different scales). The main difference are:

1. Edges chosen randomly.
2. Fixed base 2 for the buckets (which can be thought of as the different scales), in contrast to adapting them to the desired/optimal DER.
3. Much dispersion of edge intervals, within each bucket the are many values available.

In terms of our model, we could think of a fixed NER of 2 with `m` scales which gives a fixed SC independent of graph size (*fixed* here doesn't mean deterministic, rather fixed parameters that determine the *average* of the resulting NER/SC). Note that the original `DRSample` has a fixed `m = 1` but it can be easily adapted to have any value as described in the `BucketSample[n, m]` algorithm in [Scaling Proof-of-Replication for Filecoin Mining](https://web.stanford.edu/~bfisch/porep_short.pdf) [is there an alternative definition?] where we basically can think that we allow to randomly choose more than one edge from this logarithmically-partitioned distribution.

The key insight is that the asymmetries that these types of construction bring imply with them weak spots in the edge topology that can be exploited to get NERs bigger than what we would get from our Uniform graph for the same `m`. If we randomly "over-protected" a side of the graph (since the total amount of edges is fixed) another part of it will be "under-protected" (under/over means the graph will expose NERs above/below the theoretical DER of the Uniform construction and the attacker will be able to focus there). A couple of simple examples to illustrate this follow [pending, not done, will be developed more in depth once we have some diagrams which will make this much easier]. [Explain for example that a hard split has a `NER >> 1` because the nodes removed generate profitable exchanges due to their scattered distribution, in contrast with the Uniform graph which forces the removal of contiguous nodes which don't give any/much extra NER.]

The key strength of this construction is its obscured topology (through random assignments) which makes it computationally more expensive to scan for these weaknesses. The question is then if this obscurity outweighs the weakness it entails. At the moment we have been able to achieve the theoretical DER for `DRSample` of at least `2^16`, the question is how such attacks will be able to scale with graph size, but it seems the weaknesses are in fact there. Put another way, if the attacker had a cheap way to scan the entire graph it seems that the robustness of the DRSample is below the one of Uniform (but this is just theoretical to illustrate this point).

So the question now is shifted from the original "what are the constants in the bounds of the `(e,d)` pair for DRSample?" to "how computationally expensive is it to attack the DRSample with big (production-like) graph sizes to achieve a DER below the one of the Uniform graph?"

## Attack

If we wanted to attack the Uniform graph we would just adjust a block size for the intended scale and remove `NER^(scale-1)` contiguous blocks to switch to an edge on the main path of interval `NER^(scale)` and have an exchange of `NER` nodes. This had the implicit knowledge that all nodes had the same edges with the same protection, so to generate a cut of edges at a certain scale we had to remove as many nodes as the interval that edge had (plus one), to ensure no edge of that size could just "jump above" the block without resorting to higher-interval edges of the next scale.

In the DRSample we can't assume that, at any scale. We need to start by removing a single node (usually called *frontier* in the code) and examine which edges *crossed* it (another term used in the code), meaning which edges had inside their interval this node so that we had the possibility (at some point, by removing some more nodes) of generating a switch to that edge; we can't switch to an edge whose interval hasn't been interrupted (removed one or more of its nodes) as there is always the underlying IPP path providing the most length (which will always be included in the final depth).

At any point, after the removal of the frontier, we expect to win a NER equal to the interval of the smallest edge that contains it divided by one (the amount of nodes removed). If we remove that edge, by removing one its endpoint nodes, the next smallest edge will be the one that we'll switch to (in the final depth computation), yielding a NER equal to this new (and possible bigger) interval now divided by two nodes. The approach is then straightforward, we keep removing the smallest edges that contain the original frontier node and check the updated NER values to see if they surpass a certain threshold. In practice that the approach is very sensitive to that threshold (that in part depends on the theoretical DER) as values too low will mean will settle for suboptimal exchanges (NERs) and values too high will mean we'll lose good exchange opportunities. (The current attack is under heavy development and this is always subject to change.)

As explained in the [Exchange nodes](#exchange-nodes) section this is an ideal analysis and for it to hold we study only the edges contained within a local span of contiguous nodes, and once we have removed the nodes that yield an acceptable NER we don't remove from that span again to avoid interactions that will probably destroy the effectiveness of that assumption.
